{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL agent with SelectAI and Langchain Agent\n",
    "* https://github.com/cohere-ai/notebooks/blob/main/notebooks/agents/Vanilla_Tool_Use.ipynb\n",
    "* This is a better version, improved doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from langchain.tools import Tool\n",
    "\n",
    "from langchain_community.chat_models import ChatOCIGenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "from select_ai_sql_agent import SelectAISQLAgent\n",
    "from config_reader import ConfigReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "COMPARTMENT_ID = \"ocid1.compartment.oc1..aaaaaaaaushuwb2evpuf7rcpl4r7ugmqoe7ekmaiik3ra3m7gec3d234eknq\"\n",
    "\n",
    "# fopr now only Cohere Command-R plus\n",
    "LLM_MODEL = \"cohere.command-r-plus-08-2024\"\n",
    "ENDPOINT = \"https://inference.generativeai.eu-frankfurt-1.oci.oraclecloud.com\"\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "# internet search\n",
    "TAVILY_API_KEY = \"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\"\n",
    "SEARCH_ENDPOINT = \"https://api.tavily.com/search\"\n",
    "\n",
    "chat = ChatOCIGenAI(\n",
    "    auth_type=\"API_KEY\",\n",
    "    model_id=LLM_MODEL,\n",
    "    service_endpoint=ENDPOINT,\n",
    "    compartment_id=COMPARTMENT_ID,\n",
    "    model_kwargs={\"temperature\": 0, \"max_tokens\": 2048},\n",
    "    is_stream=False,\n",
    ")\n",
    "\n",
    "# will be added as SystemMessage\n",
    "preamble = \"\"\"\n",
    "## Task & Context\n",
    "You are an interactive assistant designed to help users by answering their questions and fulfilling their requests. \n",
    "Users may ask about a wide range of topics, and your goal is to provide the most accurate and helpful response possible.\n",
    "To assist you, you have access to several specialized tools that can help you research and generate answers. \n",
    "Use these tools effectively to meet the user's needs.\n",
    "\n",
    "## Tools Available\n",
    "1. **not_allowed**: Use this tool to handle requests involving operations such as DROP, DELETE, INSERT, or UPDATE in a database. These actions are prohibited.\n",
    "2. **generate_sql**: Use this tool to generate SQL queries when the request requires reading data from a database.\n",
    "3. **do_rag**: Use this tool to answer general informational questions through retrieval-augmented generation (RAG).\n",
    "4. **do_internet_search**: Use this tool only when Internet search is explicitely required or when realtime information are required.\n",
    "\n",
    "## Style Guide\n",
    "- Always respond in clear, full sentences unless the user specifies otherwise.\n",
    "- Use proper grammar and spelling in all responses.\n",
    "- Strive to be concise yet comprehensive, ensuring your answers fully address the user's needs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read general configuration\n",
    "config = ConfigReader(\"./config.toml\")\n",
    "\n",
    "sql_agent = SelectAISQLAgent(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool: Simulate RAG\n",
    "def do_rag(query: str):\n",
    "    \"\"\"\n",
    "    Responds to general information requests using an LLM model.\n",
    "    Requires one parameter:\n",
    "    - query (string): the question or information request.\n",
    "    Returns a textual response as a string.\n",
    "    \"\"\"\n",
    "    print(\"Calling do_rag...\")\n",
    "\n",
    "    # here we should add the retrieval code\n",
    "    response = chat.invoke([HumanMessage(query)])\n",
    "\n",
    "    if DEBUG:\n",
    "        print(response)\n",
    "\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Tool: Generate SQL\n",
    "def generate_sql(query: str):\n",
    "    \"\"\"\n",
    "    Generates an SQL query based on the provided user request.\n",
    "    Only READ (SELECT) operations are allowed.\n",
    "\n",
    "    Requires one parameter:\n",
    "    - query (string): the user request, a description of the data to extract.\n",
    "    Returns the data read from database as a string.\n",
    "    \"\"\"\n",
    "    print(\"Calling generate sql...\")\n",
    "\n",
    "    sql = sql_agent.generate_sql(query)\n",
    "\n",
    "    print(f\"SQL generated:\\n {sql}\")\n",
    "\n",
    "    if sql_agent.check_sql(sql):\n",
    "        rows = sql_agent.execute_sql(sql)\n",
    "        print(f\"SQL executed, passing data to LLM for answer generation...\")\n",
    "\n",
    "    return str(rows)\n",
    "\n",
    "\n",
    "def not_allowed(query: str):\n",
    "    \"\"\"\n",
    "    Generate the answer for NOT allowed requests:\n",
    "    INSERT, UPDATE, DELETE, DROP\n",
    "\n",
    "    Requires one parameter:\n",
    "    - query (string): user request.\n",
    "    Returns a message.\n",
    "    \"\"\"\n",
    "    print(\"Calling not allowed...\")\n",
    "\n",
    "    return f\"I'm sorry to inform you that your request: {query} is NOT allowed!!!\"\n",
    "\n",
    "\n",
    "def do_internet_search(query: str):\n",
    "    \"\"\"\n",
    "    Generate the answer ONLY when it is requested a search on Internet\n",
    "    \"\"\"\n",
    "    print(\"Calling do internet search...\")\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"api_key\": TAVILY_API_KEY, \"query\": query}\n",
    "\n",
    "    response = requests.post(SEARCH_ENDPOINT, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"\")\n",
    "        print(\"Search results:\")\n",
    "        print(response.json())\n",
    "        print(\"\")\n",
    "\n",
    "        return response.json()\n",
    "    else:\n",
    "        err_msg = f\"Errore: {response.status_code} - {response.text}\"\n",
    "        print(err_msg)\n",
    "        return err_msg\n",
    "\n",
    "\n",
    "do_rag_tool = Tool(\n",
    "    name=\"do_rag\",\n",
    "    func=do_rag,\n",
    "    description=\"Responds to general information requests using an LLM model. Requires one parameter: query (string). Returns the model's response as a string.\",\n",
    ")\n",
    "\n",
    "generate_sql_tool = Tool(\n",
    "    name=\"generate_sql\",\n",
    "    func=generate_sql,\n",
    "    description=\"Generates and execute SQL query based on the provided input. Requires one parameter: query (string). Returns retrieved data as a string.\",\n",
    ")\n",
    "\n",
    "not_allowed_tool = Tool(\n",
    "    name=\"not_allowed\",\n",
    "    func=not_allowed,\n",
    "    description=\"Return the answer when an action is not allowed\",\n",
    ")\n",
    "\n",
    "do_internet_search_tool = Tool(\n",
    "    name=\"do_internet_search\",\n",
    "    func=do_internet_search,\n",
    "    description=\"Return the answer when an internet search is requested\",\n",
    ")\n",
    "\n",
    "# Setup tools\n",
    "tools = [do_rag_tool, generate_sql_tool, not_allowed_tool, do_internet_search_tool]\n",
    "\n",
    "# function mapped for direct access\n",
    "functions_map = {\n",
    "    \"do_rag\": do_rag_tool,\n",
    "    \"generate_sql\": generate_sql_tool,\n",
    "    \"not_allowed\": not_allowed_tool,\n",
    "    \"do_internet_search\": do_internet_search_tool,\n",
    "}\n",
    "\n",
    "# Bind tools to chat model\n",
    "chat_with_tools = chat.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting functions\n",
    "\n",
    "\n",
    "# gives only the action chosen (which tool is called\n",
    "def test_with_tools(query):\n",
    "    messages = [SystemMessage(preamble), HumanMessage(query)]\n",
    "\n",
    "    ai_msg = chat_with_tools.invoke(messages, is_force_single_step=False)\n",
    "\n",
    "    print(\"Msg content: \", ai_msg.content)\n",
    "    print(\"\")\n",
    "    # read tool calls from model output\n",
    "    for call in ai_msg.tool_calls:\n",
    "        print(\"Tool call: \", call)\n",
    "\n",
    "\n",
    "def answer_with_tools(chat_with_tools, query):\n",
    "    # we could also add here the chat history (memory)\n",
    "    messages = [SystemMessage(preamble), HumanMessage(query)]\n",
    "\n",
    "    ai_msg = chat_with_tools.invoke(messages, is_force_single_step=False)\n",
    "\n",
    "    if DEBUG:\n",
    "        print(ai_msg)\n",
    "\n",
    "    if ai_msg is None:\n",
    "        print(\"None returned chat_with_tools call...\")\n",
    "        return \"Error in chat_with_tools call\"\n",
    "\n",
    "    messages.append(ai_msg)\n",
    "\n",
    "    if DEBUG:\n",
    "        print(ai_msg.content)\n",
    "        for call in ai_msg.tool_calls:\n",
    "            print(\"Tool call: \", call)\n",
    "\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        selected_tool = functions_map[tool_call[\"name\"].lower()]\n",
    "        tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "        messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "\n",
    "    final_response = chat_with_tools.invoke(messages)\n",
    "\n",
    "    return final_response\n",
    "\n",
    "\n",
    "def print_citations(answer):\n",
    "    if answer.additional_kwargs[\"citations\"] is not None:\n",
    "        print()\n",
    "        print(\"--- Citations ---\")\n",
    "        for cite in answer.additional_kwargs[\"citations\"]:\n",
    "            print(cite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking tools and passing tool outputs back to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# query that needs an answer from the LLM (do_rag tool)\n",
    "query = \"Who is Enrico Fermi?\"\n",
    "\n",
    "answer = answer_with_tools(chat_with_tools, query)\n",
    "print(\"\")\n",
    "print(answer.content)\n",
    "\n",
    "# print_citations(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 16:45:35,221 - Generating SQL...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling generate sql...\n",
      "SQL generated:\n",
      " SELECT T2.prod_name AS product_name, COUNT(*) AS num_sales, SUM(T1.amount_sold) AS total_amount_in_euro\n",
      "FROM \"SELAI\".\"SALES\" T1\n",
      "INNER JOIN \"SELAI\".\"PRODUCTS\" T2 ON T1.prod_id = T2.prod_id\n",
      "GROUP BY T2.prod_name\n",
      "ORDER BY total_amount_in_euro DESC\n",
      "FETCH FIRST 10 ROWS ONLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 16:45:39,301 - SQL validated. Executing...\n",
      "2024-12-11 16:45:39,804 - Executed successfully. Rows fetched: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL executed, passing data to LLM for answer generation...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Product Name | Number of Sales | Total Amount in Euro |\n",
       "|---|---|---|\n",
       "| Envoy Ambassador | 9591 | 15011642.52 |\n",
       "| Mini DV Camcorder with 3.5\" Swivel LCD | 6160 | 8314815.4 |\n",
       "| 17\" LCD w/built-in HDTV Tuner | 6010 | 7189171.77 |\n",
       "| Home Theatre Package with DVD-Audio/Video Play | 10903 | 6691996.81 |\n",
       "| 5MP Telephoto Digital Camera | 6002 | 6312268.4 |\n",
       "| Envoy 256MB - 40GB | 5766 | 5635963.08 |\n",
       "| 18\" Flat Panel Graphics Monitor | 5205 | 5498727.81 |\n",
       "| 8.3 Minitower Speaker | 7197 | 3845387.38 |\n",
       "| Unix/Windows 1-user pack | 16796 | 3543725.89 |\n",
       "| SIMM- 16MB PCMCIAII card | 15950 | 2572944.13 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 253 ms, sys: 36.8 ms, total: 290 ms\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"\"\"I want a list of the top 10 products sold.\n",
    "For each product I want the product name, number of sales and total amount in euro sold.\n",
    "Return output as markdown\"\"\"\n",
    "\n",
    "answer = answer_with_tools(chat_with_tools, query)\n",
    "\n",
    "print(\"\")\n",
    "display(Markdown(answer.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling not allowed...\n",
      "\n",
      "I'm sorry to inform you that your request: DROP SALES is NOT allowed!!!\n"
     ]
    }
   ],
   "source": [
    "query = \"Please, DROP SALES\"\n",
    "\n",
    "answer = answer_with_tools(chat_with_tools, query)\n",
    "\n",
    "print(\"\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"What are the latest news regarding Italy? Make a report\"\n",
    "\n",
    "answer = answer_with_tools(chat_with_tools, query)\n",
    "\n",
    "print(\"\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm sorry, I don't have access to the data you would like me to analyse. Could you please provide it?\n",
      "CPU times: user 25.2 ms, sys: 4.57 ms, total: 29.8 ms\n",
      "Wall time: 4.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"Analyze the data provided? Make a report\"\n",
    "\n",
    "answer = answer_with_tools(chat_with_tools, query)\n",
    "\n",
    "print(\"\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
